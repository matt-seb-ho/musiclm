{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple, Dict, Any\n",
    "\n",
    "import fadtk\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.set_device(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLAKH_DIR = Path(\"/data/matt/slakh2100_flac_redux\")\n",
    "BABYSLAKH_DIR = Path(\"/data/matt/babyslakh_16k\")\n",
    "TRACK_ID_PATTERN = re.compile(r\"slakh2100_flac_redux\\/(.+?)\\/Track(\\d+)\\/mix\\.flac$\")\n",
    "BABYSLAKH_TRACK_ID_PATTERN = re.compile(r\"\\/Track(\\d+)\\/mix\\.wav$\")\n",
    "DEFAULT_INSTRUMENTS = [\"Piano\", \"Bass\", \"Guitar\", \"Drums\"]\n",
    "DEFAULT_MIDI_TEMPO = 500000\n",
    "BABYSLAKH_SAMPLE_RATE = 16000\n",
    "SLAKH_SAMPLE_RATE = 44100\n",
    "\n",
    "\n",
    "def get_babyslakh_paths(root_dir: Path = BABYSLAKH_DIR) -> List[Path]:\n",
    "    return [\n",
    "        root_dir / track_dir / \"mix.wav\"\n",
    "        for track_dir in os.listdir(root_dir)\n",
    "        if \"Track\" in track_dir and (root_dir / track_dir / \"mix.wav\").exists()\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_slakh_paths(root_dir: Path = SLAKH_DIR) -> List[Path]:\n",
    "    splits = [\"train\", \"test\", \"validation\"]\n",
    "    paths = []\n",
    "    for split_dir in os.listdir(root_dir):\n",
    "        if split_dir not in splits:\n",
    "            continue\n",
    "        split_path = root_dir / split_dir\n",
    "        for track_dir in os.listdir(split_path):\n",
    "            mix_path = split_path / track_dir / \"mix.flac\"\n",
    "            if \"Track\" in track_dir and mix_path.exists():\n",
    "                paths.append(mix_path)\n",
    "    return paths\n",
    "\n",
    "\n",
    "def extract_sample_id(path: str, is_babyslakh: bool = False) -> Tuple[str, str]:\n",
    "    pattern = BABYSLAKH_TRACK_ID_PATTERN if is_babyslakh else TRACK_ID_PATTERN\n",
    "    match = pattern.search(path)\n",
    "    if match is None:\n",
    "        raise ValueError(f\"Track ID not found in path: {path}\")\n",
    "    if is_babyslakh:\n",
    "        coin_flip = random.randint(0, 1)\n",
    "        split = \"test\" if coin_flip == 0 else \"train\"\n",
    "        return split, match.group(1)\n",
    "    return match.group(1), match.group(2)\n",
    "\n",
    "\n",
    "def get_midi_program_names(track_directory: Path) -> List[str]:\n",
    "    try:\n",
    "        with open(track_directory / \"metadata.yaml\", \"r\") as f:\n",
    "            metadata = yaml.safe_load(f)\n",
    "        program_names = []\n",
    "        for stem_id, stem_info in metadata[\"stems\"].items():\n",
    "            if \"midi_program_name\" in stem_info:\n",
    "                program_names.append(stem_info[\"midi_program_name\"])\n",
    "        return program_names\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load metadata for {track_directory}: {e}\")\n",
    "        return DEFAULT_INSTRUMENTS\n",
    "\n",
    "\n",
    "def get_tempo(mid):\n",
    "    for track in mid.tracks:\n",
    "        for msg in track:\n",
    "            if msg.type == \"set_tempo\":\n",
    "                return msg.tempo\n",
    "    return DEFAULT_MIDI_TEMPO\n",
    "\n",
    "\n",
    "def get_bpm(track_directory: Path) -> int:\n",
    "    try:\n",
    "        mid = mido.MidiFile(track_directory / \"all_src.mid\")\n",
    "        tempo = get_tempo(mid)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get tempo for {track_directory}: {e}\")\n",
    "        tempo = DEFAULT_MIDI_TEMPO\n",
    "    return round(mido.tempo2bpm(tempo))\n",
    "\n",
    "\n",
    "def get_condition_data(slakh_paths, is_babyslakh: bool = False) -> Dict[str, Any]:\n",
    "    condition_data = defaultdict(dict)\n",
    "    for audio_path in tqdm(slakh_paths):\n",
    "        track_directory = audio_path.parent\n",
    "        path_str = str(audio_path)\n",
    "        split, track_id = extract_sample_id(path_str, is_babyslakh=is_babyslakh)\n",
    "        if split == \"train\":\n",
    "            split = \"training\"\n",
    "        try:\n",
    "            bpm = get_bpm(track_directory)\n",
    "            program_names = get_midi_program_names(track_directory)\n",
    "            condition_data[split][track_id] = {\n",
    "                \"bpm\": bpm,\n",
    "                \"midi_program_names\": program_names,\n",
    "                \"track_path\": str(audio_path),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Failed on {audio_path}: {e}\")\n",
    "    return condition_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create reference slakh subset\n",
    "test_dir = SLAKH_DIR / \"test\"\n",
    "reference_dir = Path(\"/data/matt/st_ref\")\n",
    "\n",
    "\n",
    "def create_reference_dir(\n",
    "    reference_dir: Path,\n",
    "    num_tracks: int = 32,\n",
    ") -> None:\n",
    "    reference_dir.mkdir(parents=True, exist_ok=True)\n",
    "    tracks_copied = 0\n",
    "    for track_dir in os.listdir(test_dir):\n",
    "        if \"Track\" not in track_dir:\n",
    "            continue\n",
    "        mix_flac = test_dir / track_dir / \"mix.flac\"\n",
    "        if not mix_flac.exists():\n",
    "            continue\n",
    "        audio, original_sr = librosa.load(mix_flac, sr=None, mono=False)\n",
    "        resampled = librosa.resample(audio, orig_sr=original_sr, target_sr=32000)\n",
    "        if resampled.ndim == 2:\n",
    "            # librosa returns (channels, samples), sf expects (samples, channels)\n",
    "            resampled = resampled.T\n",
    "\n",
    "        _, track_id = extract_sample_id(str(mix_flac))\n",
    "        destination = reference_dir / f\"track{track_id}.wav\"\n",
    "        sf.write(destination, resampled, samplerate=32000)\n",
    "        tracks_copied += 1\n",
    "        if tracks_copied >= num_tracks:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_reference_dir(reference_dir, num_tracks=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Loading HTSAT-base model config.\n",
      "/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the specified checkpoint /data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/fadtk/.model-checkpoints/music_audioset_epoch_15_esc_90.14.pt from users.\n",
      "Load Checkpoint...\n",
      "logit_scale_a \t Loaded\n",
      "logit_scale_t \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_real.weight \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_imag.weight \t Loaded\n",
      "audio_branch.logmel_extractor.melW \t Loaded\n",
      "audio_branch.bn0.weight \t Loaded\n",
      "audio_branch.bn0.bias \t Loaded\n",
      "audio_branch.patch_embed.proj.weight \t Loaded\n",
      "audio_branch.patch_embed.proj.bias \t Loaded\n",
      "audio_branch.patch_embed.norm.weight \t Loaded\n",
      "audio_branch.patch_embed.norm.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.norm.weight \t Loaded\n",
      "audio_branch.norm.bias \t Loaded\n",
      "audio_branch.tscam_conv.weight \t Loaded\n",
      "audio_branch.tscam_conv.bias \t Loaded\n",
      "audio_branch.head.weight \t Loaded\n",
      "audio_branch.head.bias \t Loaded\n",
      "text_branch.embeddings.word_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.position_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.token_type_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.bias \t Loaded\n",
      "text_branch.pooler.dense.weight \t Loaded\n",
      "text_branch.pooler.dense.bias \t Loaded\n",
      "text_transform.sequential.0.weight \t Loaded\n",
      "text_transform.sequential.0.bias \t Loaded\n",
      "text_transform.sequential.3.weight \t Loaded\n",
      "text_transform.sequential.3.bias \t Loaded\n",
      "text_projection.0.weight \t Loaded\n",
      "text_projection.0.bias \t Loaded\n",
      "text_projection.2.weight \t Loaded\n",
      "text_projection.2.bias \t Loaded\n",
      "audio_transform.sequential.0.weight \t Loaded\n",
      "audio_transform.sequential.0.bias \t Loaded\n",
      "audio_transform.sequential.3.weight \t Loaded\n",
      "audio_transform.sequential.3.bias \t Loaded\n",
      "audio_projection.0.weight \t Loaded\n",
      "audio_projection.0.bias \t Loaded\n",
      "audio_projection.2.weight \t Loaded\n",
      "audio_projection.2.bias \t Loaded\n"
     ]
    }
   ],
   "source": [
    "embedding_model = fadtk.CLAPLaionModel(\"music\")\n",
    "fad = fadtk.FrechetAudioDistance(\n",
    "    ml=embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_OUTPUT_DIR = Path(\"/data/matt/mg_baseline_output\")\n",
    "FINETUNE_OUTPUT_DIR = Path(\"/data/matt/mg_finetune_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_0.wav   sample_16.wav  sample_22.wav  sample_29.wav  sample_6.wav\n",
      "sample_10.wav  sample_17.wav  sample_23.wav  sample_2.wav   sample_7.wav\n",
      "sample_11.wav  sample_18.wav  sample_24.wav  sample_30.wav  sample_8.wav\n",
      "sample_12.wav  sample_19.wav  sample_25.wav  sample_31.wav  sample_9.wav\n",
      "sample_13.wav  sample_1.wav   sample_26.wav  sample_3.wav\n",
      "sample_14.wav  sample_20.wav  sample_27.wav  sample_4.wav\n",
      "sample_15.wav  sample_21.wav  sample_28.wav  sample_5.wav\n"
     ]
    }
   ],
   "source": [
    "!ls {BASELINE_OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Frechet Audio Distance] Loading 32 audio files...\n",
      "/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Loading HTSAT-base model config.\n",
      "Loading HTSAT-base model config.\n",
      "Loading HTSAT-base model config.\n",
      "Loading HTSAT-base model config.\n",
      "Loading HTSAT-base model config.\n",
      "Loading HTSAT-base model config.\n",
      "Loading HTSAT-base model config.\n",
      "Loading HTSAT-base model config.\n",
      "/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 44.52 GiB of which 15.00 MiB is free. Process 94591 has 37.20 GiB memory in use. Process 248927 has 576.00 MiB memory in use. Process 286967 has 576.00 MiB memory in use. Process 287044 has 576.00 MiB memory in use. Process 287123 has 576.00 MiB memory in use. Process 287202 has 576.00 MiB memory in use. Process 287279 has 576.00 MiB memory in use. Process 288203 has 574.00 MiB memory in use. Process 288362 has 574.00 MiB memory in use. Process 288842 has 574.00 MiB memory in use. Process 298990 has 1.20 GiB memory in use. Including non-PyTorch memory, this process has 1.00 GiB memory in use. Of the allocated memory 577.03 MiB is allocated by PyTorch, and 24.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRemoteTraceback\u001b[39m                           Traceback (most recent call last)",
      "\u001b[31mRemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/data/matt/miniconda3/envs/fadenv/lib/python3.12/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/data/matt/miniconda3/envs/fadenv/lib/python3.12/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n           ^^^^^^^^^^^^^^^^\n  File \"/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/fadtk/fad_batch.py\", line 19, in _cache_embedding_batch\n    fad = FrechetAudioDistance(ml, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/fadtk/fad.py\", line 133, in __init__\n    self.ml.load_model()\n  File \"/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/fadtk/model_loader.py\", line 385, in load_model\n    self.model = laion_clap.CLAP_Module(enable_fusion=False, amodel='HTSAT-tiny' if self.type == 'audio' else 'HTSAT-base')\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/laion_clap/hook.py\", line 52, in __init__\n    model, model_cfg = create_model(\n                       ^^^^^^^^^^^^^\n  File \"/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/laion_clap/clap_module/factory.py\", line 219, in create_model\n    model.to(device=device)\n  File \"/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1355, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 915, in _apply\n    module._apply(fn)\n  File \"/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 915, in _apply\n    module._apply(fn)\n  File \"/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 915, in _apply\n    module._apply(fn)\n  [Previous line repeated 3 more times]\n  File \"/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 942, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1341, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 44.52 GiB of which 15.00 MiB is free. Process 94591 has 37.20 GiB memory in use. Process 248927 has 576.00 MiB memory in use. Process 286967 has 576.00 MiB memory in use. Process 287044 has 576.00 MiB memory in use. Process 287123 has 576.00 MiB memory in use. Process 287202 has 576.00 MiB memory in use. Process 287279 has 576.00 MiB memory in use. Process 288203 has 574.00 MiB memory in use. Process 288362 has 574.00 MiB memory in use. Process 288842 has 574.00 MiB memory in use. Process 298990 has 1.20 GiB memory in use. Including non-PyTorch memory, this process has 1.00 GiB memory in use. Of the allocated memory 577.03 MiB is allocated by PyTorch, and 24.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dir_ \u001b[38;5;129;01min\u001b[39;00m [BASELINE_OUTPUT_DIR, FINETUNE_OUTPUT_DIR, reference_dir]:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mfadtk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache_embedding_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdir_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/fadtk/fad_batch.py:48\u001b[39m, in \u001b[36mcache_embedding_files\u001b[39m\u001b[34m(files, ml, workers, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m multiprocessing.set_start_method(\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m, force=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.multiprocessing.Pool(workers) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_cache_embedding_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/matt/miniconda3/envs/fadenv/lib/python3.12/multiprocessing/pool.py:367\u001b[39m, in \u001b[36mPool.map\u001b[39m\u001b[34m(self, func, iterable, chunksize)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    363\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[33;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[32m    365\u001b[39m \u001b[33;03m    in a list that is returned.\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/matt/miniconda3/envs/fadenv/lib/python3.12/multiprocessing/pool.py:774\u001b[39m, in \u001b[36mApplyResult.get\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 44.52 GiB of which 15.00 MiB is free. Process 94591 has 37.20 GiB memory in use. Process 248927 has 576.00 MiB memory in use. Process 286967 has 576.00 MiB memory in use. Process 287044 has 576.00 MiB memory in use. Process 287123 has 576.00 MiB memory in use. Process 287202 has 576.00 MiB memory in use. Process 287279 has 576.00 MiB memory in use. Process 288203 has 574.00 MiB memory in use. Process 288362 has 574.00 MiB memory in use. Process 288842 has 574.00 MiB memory in use. Process 298990 has 1.20 GiB memory in use. Including non-PyTorch memory, this process has 1.00 GiB memory in use. Of the allocated memory 577.03 MiB is allocated by PyTorch, and 24.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "for dir_ in [BASELINE_OUTPUT_DIR, FINETUNE_OUTPUT_DIR, reference_dir]:\n",
    "    fadtk.cache_embedding_files(\n",
    "        dir_,\n",
    "        embedding_model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading embedding files from /data/matt/st_ref...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "No files provided",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m baseline_fad_score = \u001b[43mfad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreference_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mBASELINE_OUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(baseline_fad_score)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/fadtk/fad.py:299\u001b[39m, in \u001b[36mFrechetAudioDistance.score\u001b[39m\u001b[34m(self, baseline, eval)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscore\u001b[39m(\u001b[38;5;28mself\u001b[39m, baseline: PathLike, \u001b[38;5;28meval\u001b[39m: PathLike):\n\u001b[32m    293\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[33;03m    Calculate a single FAD score between a background and an eval set.\u001b[39;00m\n\u001b[32m    295\u001b[39m \n\u001b[32m    296\u001b[39m \u001b[33;03m    :param baseline: Baseline matrix or directory containing baseline audio files\u001b[39;00m\n\u001b[32m    297\u001b[39m \u001b[33;03m    :param eval: Eval matrix or directory containing eval audio files\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m     mu_bg, cov_bg = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m     mu_eval, cov_eval = \u001b[38;5;28mself\u001b[39m.load_stats(\u001b[38;5;28meval\u001b[39m)\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m calc_frechet_distance(mu_bg, cov_bg, mu_eval, cov_eval)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/fadtk/fad.py:282\u001b[39m, in \u001b[36mFrechetAudioDistance.load_stats\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m    278\u001b[39m     exit(\u001b[32m1\u001b[39m)\n\u001b[32m    280\u001b[39m log.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading embedding files from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m mu, cov = \u001b[43mcalculate_embd_statistics_online\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43memb_dir\u001b[49m\u001b[43m.\u001b[49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*.npy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m log.info(\u001b[33m\"\u001b[39m\u001b[33m> Embeddings statistics calculated.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    285\u001b[39m \u001b[38;5;66;03m# Save statistics\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/matt/miniconda3/envs/fadenv/lib/python3.12/site-packages/fadtk/utils.py:25\u001b[39m, in \u001b[36mcalculate_embd_statistics_online\u001b[39m\u001b[34m(files)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalculate_embd_statistics_online\u001b[39m(files: \u001b[38;5;28mlist\u001b[39m[PathLike]) -> \u001b[38;5;28mtuple\u001b[39m[np.ndarray, np.ndarray]:\n\u001b[32m     20\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[33;03m    Calculate the mean and covariance matrix of a list of embeddings in an online manner.\u001b[39;00m\n\u001b[32m     22\u001b[39m \n\u001b[32m     23\u001b[39m \u001b[33;03m    :param files: A list of npy files containing ndarrays with shape (n_frames, n_features)\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(files) > \u001b[32m0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mNo files provided\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# Load the first file to get the embedding dimension\u001b[39;00m\n\u001b[32m     28\u001b[39m     embd_dim = np.load(files[\u001b[32m0\u001b[39m]).shape[-\u001b[32m1\u001b[39m]\n",
      "\u001b[31mAssertionError\u001b[39m: No files provided"
     ]
    }
   ],
   "source": [
    "baseline_fad_score = fad.score(\n",
    "    reference_dir,\n",
    "    BASELINE_OUTPUT_DIR,\n",
    ")\n",
    "print(baseline_fad_score)\n",
    "\n",
    "finetune_score = fad.score(\n",
    "    reference_dir,\n",
    "    FINETUNE_OUTPUT_DIR,\n",
    ")\n",
    "print(finetune_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fadenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
